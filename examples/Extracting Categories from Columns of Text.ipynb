{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3efa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import catllm as cat\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bc20ee",
   "metadata": {},
   "source": [
    "Here we set our API's. I'm reading from a file on my machine, but you can also just copy and paste the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6538fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_api_key = \"ANTHROPIC_API_KEY\"\n",
    "openai_api_key = \"OPENAI_API_KEY\"\n",
    "preplexity_api_key = \"PERPLEXITY_API_KEY\"\n",
    "mistral_api_key = \"MISTRAL_API_KEY\"\n",
    "google_api_key = \"GOOGLE_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca5227",
   "metadata": {},
   "source": [
    "Generating some fake data and categories to assign to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70e9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data we will be categoizing\n",
    "list_names = [\"becuase i dont like living here\", \"for a bigger house\", \"to be with my wife\"]\n",
    "\n",
    "#categories we want to extract from the data\n",
    "user_categories = [\"to start living with or to stay with partner/spouse\",\n",
    "                   \"relationship change (divorce, breakup, etc)\",\n",
    "                   \"the person had a job or school or career change, including transferred and retired\",\n",
    "                   \"the person's partner's job or school or career change, including transferred and retired\",\n",
    "                   \"financial reasons (rent is too expensive, pay raise, etc)\",\n",
    "                   \"related specifically features of the home, such as a bigger or smaller yard\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cfd3e3",
   "metadata": {},
   "source": [
    "Below we're running the most simple example of the package. The essential elements are defined above, which are \n",
    "1. The survey input you want to categorize (a column of text data)\n",
    "2. The categories you want to extract (defined by you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201cdfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anthropic = cat.multi_class(\n",
    "    survey_input= list_names, \n",
    "    user_model=\"claude-sonnet-4-20250514\",\n",
    "    categories=user_categories,\n",
    "    api_key=anthropic_api_key)\n",
    "\n",
    "test_anthropic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0441682",
   "metadata": {},
   "source": [
    "The package will automatically detect when you switch model source. Below, we're running an example with GPT-5. Just remember to make sure you input the correct API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb9cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_openai = cat.multi_class(\n",
    "    survey_question=\"why did you move?\", \n",
    "    survey_input= list_names, \n",
    "    user_model=\"gpt-5\",\n",
    "    categories=user_categories,\n",
    "    api_key=openai_api_key)\n",
    "\n",
    "test_openai.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6439c2",
   "metadata": {},
   "source": [
    "One of the most powerful features of CatLLM is the ability to call on the thousands of models available on Huggingface. Here, you might need to be more explicit about where the model is coming.\n",
    "\n",
    "Here, we set model_source to \"Huggingface\" so that CatLLM knows where to pull from. Again, make sure you have the correct API key! \n",
    "\n",
    "WARNING: Using Huggingface without a Hugginface pro subscription is not recommended. The free tier will only allow you to cycle through a few rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e0199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_huggingface = cat.multi_class(\n",
    "    survey_input= list_names, \n",
    "    user_model=\"meta-llama/Llama-4-Maverick-17B-128E-Instruct:groq\",\n",
    "    model_source=\"Huggingface\",\n",
    "    categories=user_categories,\n",
    "    api_key=huggingface_api_key)\n",
    "\n",
    "test_huggingface.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e23121",
   "metadata": {},
   "source": [
    "Let's add a bit more complexity for better results. \n",
    "\n",
    "Rather than just giving the model the task of categorizing without any context, let's provide CatLLM with the survey question that was asked of the respondent. \n",
    "\n",
    "Let's also ask CatLLM to provide a bit more context on what its role and goal is. \n",
    "\n",
    "Keep in mind that these features make for a longer prompt, which translates to higher costs from the model provider. Only use this feature if you want to improve the quality of your output and you're willing to pay a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e273a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anthropic_with_context = cat.multi_class(\n",
    "    survey_input= list_names, \n",
    "    survey_question = \"Why did you move?\", # add your survey question here\n",
    "    context_prompt = True, # ask Cat-LLM to provide a bit more context on the cateorization task\n",
    "    user_model=\"claude-sonnet-4-20250514\",\n",
    "    categories=user_categories,\n",
    "    api_key=anthropic_api_key)\n",
    "\n",
    "test_anthropic_with_context.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad918d",
   "metadata": {},
   "source": [
    "Now, let's add even more complexity. Before categorizing, let's ask the model to take a \"step back\" and observe the bigger picture here. \n",
    "\n",
    "In this case, we will ask the model to consider broader reasons for why people move so that we can get it to reason towards a better answer. \n",
    "\n",
    "Again, this will increase your prompt size, so use only if you don't mind the additional API costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba7d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anthropic_step_back = cat.multi_class(\n",
    "    survey_input= list_names, \n",
    "    survey_question = \"Why did you move?\",\n",
    "    context_prompt = True,\n",
    "    step_back_prompt = True, # ask the model to consider the broader conceptual background\n",
    "    user_model=\"claude-sonnet-4-20250514\",\n",
    "    categories=user_categories,\n",
    "    api_key=anthropic_api_key)\n",
    "\n",
    "test_anthropic_step_back.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633f59f0",
   "metadata": {},
   "source": [
    "The step back method works to improve the quality of your categorizations, but chain of verification (CoVe) should improve them even further. \n",
    "\n",
    "What is CoVe? Here, we use multiple prompts to get the model to \"reason\" through its response by considering more than surface level information. That is, we ask the model a series of \"verification\" questions to get the model to think a bit more about its initial answer and have the opportunity to revise it.\n",
    "\n",
    "WARNING: Although we can combine all of these features, doing so will multiply your costs. Only combine all features if you're hoping for the absolute best output and don't mind paying 10 times as much for a small improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286cdb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anthropic_cove = cat.multi_class(\n",
    "    survey_input= list_names, \n",
    "    survey_question = \"Why did you move?\",\n",
    "    context_prompt = True,\n",
    "    step_back_prompt = True,\n",
    "    chain_of_verification = True, # allow the model to think through its initial response and change its answer\n",
    "    user_model=\"claude-sonnet-4-20250514\",\n",
    "    categories=user_categories,\n",
    "    api_key=anthropic_api_key)\n",
    "\n",
    "test_anthropic_cove.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f45752",
   "metadata": {},
   "source": [
    "Sometimes, however, a simpler prompt might be able to get us all the way there. Turning on Chain-of-Thought (CoT) can improve results without needing to do many prompts and increases costs at a fraction of the price of CoVe.\n",
    "\n",
    "CoT essentially asks the model to reson through its response a bit more before outputting a response. It does so within the same prompt, rather than needing many prompts.\n",
    "\n",
    "Below is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3319c693",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anthropic_cot = cat.multi_class(\n",
    "    survey_input= list_names, \n",
    "    survey_question = \"Why did you move?\",\n",
    "    context_prompt = True,\n",
    "    step_back_prompt = True,\n",
    "    chain_of_thought = True, # ask the model to \"reason\" through its response in the same prompt\n",
    "    user_model=\"claude-sonnet-4-20250514\",\n",
    "    categories=user_categories,\n",
    "    api_key=anthropic_api_key)\n",
    "\n",
    "test_anthropic_cot.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
