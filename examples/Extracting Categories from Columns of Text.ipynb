{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3efa2d",
   "metadata": {},
   "outputs": [],
   "source": "import catllm as cat\nimport pandas as pd\nimport os"
  },
  {
   "cell_type": "markdown",
   "id": "c9bc20ee",
   "metadata": {},
   "source": [
    "Here we set our API's. I'm reading from a file on my machine, but you can also just copy and paste the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6538fd91",
   "metadata": {},
   "outputs": [],
   "source": "# Replace these with your actual API keys\nanthropic_api_key = \"ANTHROPIC_API_KEY\"\nopenai_api_key = \"OPENAI_API_KEY\"\nperplexity_api_key = \"PERPLEXITY_API_KEY\"\nmistral_api_key = \"MISTRAL_API_KEY\"\ngoogle_api_key = \"GOOGLE_API_KEY\"\nhuggingface_api_key = \"HUGGINGFACE_API_KEY\"\nxai_api_key = \"XAI_API_KEY\""
  },
  {
   "cell_type": "markdown",
   "id": "91ca5227",
   "metadata": {},
   "source": [
    "Generating some fake data and categories to assign to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70e9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data we will be categoizing\n",
    "list_names = [\"becuase i dont like living here\", \"for a bigger house\", \"to be with my wife\"]\n",
    "\n",
    "#categories we want to extract from the data\n",
    "user_categories = [\"to start living with or to stay with partner/spouse\",\n",
    "                   \"relationship change (divorce, breakup, etc)\",\n",
    "                   \"the person had a job or school or career change, including transferred and retired\",\n",
    "                   \"the person's partner's job or school or career change, including transferred and retired\",\n",
    "                   \"financial reasons (rent is too expensive, pay raise, etc)\",\n",
    "                   \"related specifically features of the home, such as a bigger or smaller yard\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cfd3e3",
   "metadata": {},
   "source": [
    "Below we're running the most simple example of the package. The essential elements are defined above, which are \n",
    "1. The survey input you want to categorize (a column of text data)\n",
    "2. The categories you want to extract (defined by you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201cdfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anthropic = cat.multi_class(\n",
    "    survey_input= list_names, \n",
    "    user_model=\"claude-sonnet-4-20250514\",\n",
    "    categories=user_categories,\n",
    "    api_key=anthropic_api_key)\n",
    "\n",
    "test_anthropic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0441682",
   "metadata": {},
   "source": [
    "The package will automatically detect when you switch model source. Below, we're running an example with GPT-5. Just remember to make sure you input the correct API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb9cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_openai = cat.multi_class(\n",
    "    survey_question=\"why did you move?\", \n",
    "    survey_input= list_names, \n",
    "    user_model=\"gpt-5\",\n",
    "    categories=user_categories,\n",
    "    api_key=openai_api_key)\n",
    "\n",
    "test_openai.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6439c2",
   "metadata": {},
   "source": [
    "One of the most powerful features of CatLLM is the ability to call on the thousands of models available on Huggingface. Here, you might need to be more explicit about where the model is coming.\n",
    "\n",
    "Here, we set model_source to \"Huggingface\" so that CatLLM knows where to pull from. Again, make sure you have the correct API key! \n",
    "\n",
    "WARNING: Using Huggingface without a Hugginface pro subscription is not recommended. The free tier will only allow you to cycle through a few rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e0199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_huggingface = cat.multi_class(\n",
    "    survey_input= list_names, \n",
    "    user_model=\"meta-llama/Llama-4-Maverick-17B-128E-Instruct:groq\",\n",
    "    model_source=\"Huggingface\",\n",
    "    categories=user_categories,\n",
    "    api_key=huggingface_api_key)\n",
    "\n",
    "test_huggingface.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e23121",
   "metadata": {},
   "source": [
    "Let's add a bit more complexity for better results. \n",
    "\n",
    "Rather than just giving the model the task of categorizing without any context, let's provide CatLLM with the survey question that was asked of the respondent. \n",
    "\n",
    "Let's also ask CatLLM to provide a bit more context on what its role and goal is. \n",
    "\n",
    "Keep in mind that these features make for a longer prompt, which translates to higher costs from the model provider. Only use this feature if you want to improve the quality of your output and you're willing to pay a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e273a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anthropic_with_context = cat.multi_class(\n",
    "    survey_input= list_names, \n",
    "    survey_question = \"Why did you move?\", # add your survey question here\n",
    "    context_prompt = True, # ask Cat-LLM to provide a bit more context on the cateorization task\n",
    "    user_model=\"claude-sonnet-4-20250514\",\n",
    "    categories=user_categories,\n",
    "    api_key=anthropic_api_key)\n",
    "\n",
    "test_anthropic_with_context.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad918d",
   "metadata": {},
   "source": [
    "Now, let's add even more complexity. Before categorizing, let's ask the model to take a \"step back\" and observe the bigger picture here. \n",
    "\n",
    "In this case, we will ask the model to consider broader reasons for why people move so that we can get it to reason towards a better answer. \n",
    "\n",
    "Again, this will increase your prompt size, so use only if you don't mind the additional API costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba7d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anthropic_step_back = cat.multi_class(\n",
    "    survey_input= list_names, \n",
    "    survey_question = \"Why did you move?\",\n",
    "    context_prompt = True,\n",
    "    step_back_prompt = True, # ask the model to consider the broader conceptual background\n",
    "    user_model=\"claude-sonnet-4-20250514\",\n",
    "    categories=user_categories,\n",
    "    api_key=anthropic_api_key)\n",
    "\n",
    "test_anthropic_step_back.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633f59f0",
   "metadata": {},
   "source": [
    "The step back method works to improve the quality of your categorizations, but chain of verification (CoVe) should improve them even further. \n",
    "\n",
    "What is CoVe? Here, we use multiple prompts to get the model to \"reason\" through its response by considering more than surface level information. That is, we ask the model a series of \"verification\" questions to get the model to think a bit more about its initial answer and have the opportunity to revise it.\n",
    "\n",
    "WARNING: Although we can combine all of these features, doing so will multiply your costs. Only combine all features if you're hoping for the absolute best output and don't mind paying 10 times as much for a small improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286cdb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_anthropic_cove = cat.multi_class(\n",
    "    survey_input= list_names, \n",
    "    survey_question = \"Why did you move?\",\n",
    "    context_prompt = True,\n",
    "    step_back_prompt = True,\n",
    "    chain_of_verification = True, # allow the model to think through its initial response and change its answer\n",
    "    user_model=\"claude-sonnet-4-20250514\",\n",
    "    categories=user_categories,\n",
    "    api_key=anthropic_api_key)\n",
    "\n",
    "test_anthropic_cove.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f45752",
   "metadata": {},
   "source": "## Chain-of-Thought (CoT) - Now the Default\n\nSometimes, a simpler prompt might be able to get us all the way there. Chain-of-Thought (CoT) can improve results without needing multiple prompts and increases costs at a fraction of the price of CoVe.\n\nCoT essentially asks the model to reason through its response a bit more before outputting a response. It does so within the same prompt, rather than needing many prompts.\n\n**Note:** As of the latest version, `chain_of_thought=True` is now the **default setting**. This means all the examples above were already using CoT! If you want to disable it (for faster/cheaper processing), you can set `chain_of_thought=False`.\n\nBelow is an example where we explicitly disable CoT to show the difference:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3319c693",
   "metadata": {},
   "outputs": [],
   "source": "# Example with CoT explicitly DISABLED to show the difference\ntest_anthropic_no_cot = cat.multi_class(\n    survey_input=list_names, \n    survey_question=\"Why did you move?\",\n    chain_of_thought=False,  # Disable CoT for faster/cheaper processing\n    user_model=\"claude-sonnet-4-20250514\",\n    categories=user_categories,\n    api_key=anthropic_api_key)\n\ntest_anthropic_no_cot.head()"
  },
  {
   "cell_type": "markdown",
   "id": "4cec208f",
   "metadata": {},
   "source": [
    "Lastly, CatLLM can also generate categories for you by extracting the most common categories from your column of survey responses. \n",
    "\n",
    "Here, we set categories to \"auto\" and let the model identify the most important/common categories present in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d58f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/chrissoria/Documents/Research/empirical_investigation_llm/data/processed/'\n",
    "UCNets_Survey_Data = pd.read_csv(data_path+\"unique_main_a19i.csv\")\n",
    "\n",
    "test_anthropic_auto_categories = cat.multi_class(\n",
    "    survey_input= UCNets_Survey_Data['Response'], \n",
    "    survey_question = \"Why did you move?\",\n",
    "    user_model=\"claude-sonnet-4-20250514\",\n",
    "    categories=\"auto\",\n",
    "    api_key=anthropic_api_key)\n",
    "\n",
    "test_anthropic_auto_categories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apkox7rnjci",
   "source": "## Safety Feature: Incremental Saves\n\nWhen processing large datasets, API calls can fail midway through. The `safety` feature saves your progress after each API call, so you don't lose your work if something goes wrong.\n\n**How it works:**\n- Set `safety=True` and provide a `filename`\n- CatLLM will save a CSV file after processing each row\n- If the process fails, you can resume from where you left off\n\nThis is especially useful for:\n- Large datasets (hundreds or thousands of rows)\n- Expensive API calls (CoVe, step-back prompting)\n- Unstable network connections",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "1ze9d90ejhhi",
   "source": "# Example with safety feature enabled\ntest_with_safety = cat.multi_class(\n    survey_input=list_names, \n    survey_question=\"Why did you move?\",\n    user_model=\"gpt-4o\",\n    categories=user_categories,\n    safety=True,  # Enable incremental saves\n    filename=\"categorized_results.csv\",  # Required when safety=True\n    save_directory=\"./output\",  # Optional: specify output directory\n    api_key=openai_api_key)\n\ntest_with_safety.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6yt0pjfoddt",
   "source": "## Google Models with Extended Thinking (thinking_budget)\n\nGoogle's Gemini models support an extended thinking feature through the `thinking_budget` parameter. This allows the model to \"think longer\" before responding, potentially improving accuracy for complex categorization tasks.\n\n**Note:** This feature is only available for Google models and requires a Google AI Studio API key. The free tier has severe rate limits (5 requests per minute), so a billing account is recommended for large-scale use.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "jqvtdzc6i29",
   "source": "# Example with Google Gemini and extended thinking\ntest_google_thinking = cat.multi_class(\n    survey_input=list_names, \n    survey_question=\"Why did you move?\",\n    user_model=\"gemini-2.0-flash\",  # Google Gemini model\n    categories=user_categories,\n    thinking_budget=1024,  # Allow extended thinking (tokens for reasoning)\n    api_key=google_api_key)\n\ntest_google_thinking.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "t7keapbiekg",
   "source": "## Few-Shot Learning with Examples\n\nYou can provide up to 6 examples to help guide the model's categorization. This is especially useful when:\n- Your categories have nuanced definitions\n- You want consistent interpretation across responses\n- The model is making systematic errors\n\nEach example should show how a response maps to categories.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "y9ndn2lfkca",
   "source": "# Example with few-shot learning\ntest_with_examples = cat.multi_class(\n    survey_input=list_names, \n    survey_question=\"Why did you move?\",\n    user_model=\"gpt-4o\",\n    categories=user_categories,\n    # Provide examples to guide categorization\n    example1=\"'I moved to be closer to my girlfriend' -> category 1 (partner/spouse)\",\n    example2=\"'Got a new job in Seattle' -> category 3 (job/career change)\",\n    example3=\"'Rent was too expensive' -> category 5 (financial reasons)\",\n    example4=\"'Wanted a bigger backyard for the kids' -> category 6 (home features)\",\n    api_key=openai_api_key)\n\ntest_with_examples.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f3sg5uksdz",
   "source": "---\n\n# Image Classification with image_multi_class\n\nCatLLM also supports multi-label image classification. The `image_multi_class` function works similarly to `multi_class` but processes images instead of text.\n\n**Key features:**\n- Accepts a list of image file paths or a folder path\n- Supports the same prompt strategies (CoT, CoVe, step-back, context)\n- Works with vision-capable models from OpenAI, Anthropic, Google, and Mistral",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "wip0y8xrr6r",
   "source": "# Define image categories\nimage_categories = [\n    \"contains a person\",\n    \"outdoor scene\",\n    \"contains text or writing\",\n    \"has animals\",\n    \"is a photograph (not illustration)\"\n]\n\n# List of image paths (replace with your actual paths)\nimage_paths = [\n    \"path/to/image1.jpg\",\n    \"path/to/image2.png\",\n    \"path/to/image3.jpeg\"\n]\n\n# Basic image classification\nimage_results = cat.image_multi_class(\n    image_description=\"Various photographs and images\",\n    image_input=image_paths,  # Can also pass a folder path\n    categories=image_categories,\n    user_model=\"gpt-4o\",\n    api_key=openai_api_key)\n\nimage_results.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "70vk4qqhdde",
   "source": "## Image Classification with Advanced Prompt Strategies\n\nJust like text classification, you can use all the prompt enhancement strategies with images:\n\n- **chain_of_thought** (default=True): Step-by-step visual analysis\n- **chain_of_verification**: Re-examines the image to verify categorization\n- **step_back_prompt**: Analyzes key visual features before classification\n- **context_prompt**: Adds expert visual analyst role",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "8tsjdk6sb69",
   "source": "# Image classification with Chain of Verification\n# The model will re-examine the image to verify its initial categorization\nimage_results_cove = cat.image_multi_class(\n    image_description=\"Various photographs and images\",\n    image_input=image_paths,\n    categories=image_categories,\n    user_model=\"gpt-4o\",\n    chain_of_verification=True,  # Re-examine image to verify\n    context_prompt=True,  # Add expert analyst role\n    api_key=openai_api_key)\n\nimage_results_cove.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "osmv3bff1sh",
   "source": "## Image Classification from a Folder\n\nInstead of listing individual file paths, you can pass a folder path and CatLLM will automatically find all supported image files (PNG, JPG, JPEG, GIF, WebP, etc.).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vqx9ytfw85",
   "source": "# Classify all images in a folder\nfolder_path = \"path/to/your/images/folder\"\n\nimage_results_folder = cat.image_multi_class(\n    image_description=\"Collection of images to categorize\",\n    image_input=folder_path,  # Pass folder path instead of list\n    categories=image_categories,\n    user_model=\"claude-sonnet-4-20250514\",  # Works with Anthropic too\n    safety=True,  # Save progress incrementally\n    filename=\"image_classification_results.csv\",\n    api_key=anthropic_api_key)\n\nimage_results_folder.head()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1ael0j8dq0h",
   "source": "## Summary: Prompt Strategy Comparison\n\n| Strategy | Cost Impact | When to Use |\n|----------|-------------|-------------|\n| `chain_of_thought=True` (default) | Low (+10-20%) | Always - provides step-by-step reasoning |\n| `context_prompt=True` | Low (+5-10%) | When you want expert-level analysis |\n| `step_back_prompt=True` | Medium (+30-50%) | Complex categorization with subtle distinctions |\n| `chain_of_verification=True` | High (+300-500%) | When accuracy is critical, budget allows |\n| `thinking_budget` (Google only) | Medium-High | Complex reasoning tasks with Gemini models |\n\n**Recommendation:** Start with the defaults (`chain_of_thought=True`) and add more strategies only if you need improved accuracy and can afford the additional API costs.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}