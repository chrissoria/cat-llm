% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/classify.R
\name{classify}
\alias{classify}
\title{Classify text, images, or PDFs using LLMs}
\usage{
classify(
  input_data,
  categories,
  api_key = NULL,
  description = "",
  user_model = "gpt-4o",
  mode = "image",
  creativity = NULL,
  safety = FALSE,
  chain_of_verification = FALSE,
  chain_of_thought = FALSE,
  step_back_prompt = FALSE,
  context_prompt = FALSE,
  thinking_budget = 0L,
  example1 = NULL,
  example2 = NULL,
  example3 = NULL,
  example4 = NULL,
  example5 = NULL,
  example6 = NULL,
  filename = NULL,
  save_directory = NULL,
  model_source = "auto",
  max_categories = 12L,
  categories_per_chunk = 10L,
  divisions = 10L,
  research_question = NULL,
  models = NULL,
  consensus_threshold = "unanimous",
  survey_question = "",
  use_json_schema = TRUE,
  max_workers = NULL,
  fail_strategy = "partial",
  max_retries = 5L,
  batch_retries = 2L,
  retry_delay = 1,
  row_delay = 0,
  pdf_dpi = 150L,
  auto_download = FALSE,
  add_other = "prompt",
  check_verbosity = TRUE
)
}
\arguments{
\item{input_data}{A character vector, list of text strings, or
\code{data.frame} column containing the items to classify. For image or PDF
classification, a directory path or character vector of file paths.}

\item{categories}{A character vector of category names, or \code{"auto"} to
infer categories from the data (requires \code{survey_question}).}

\item{api_key}{API key for the model provider (single-model mode).
Not required when \code{models} is supplied.}

\item{description}{Character. Context description for the classification
task (e.g., the survey question or image subject). Default \code{""}.}

\item{user_model}{Character. Model name to use in single-model mode.
Default \code{"gpt-4o"}.}

\item{mode}{Character. PDF processing mode: \code{"image"} (default), \code{"text"},
or \code{"both"}.}

\item{creativity}{Numeric or \code{NULL}. Temperature setting (0–2). \code{NULL} uses
the provider default. Default \code{NULL}.}

\item{safety}{Logical. If \code{TRUE}, saves progress after each item. Default
\code{FALSE}.}

\item{chain_of_verification}{Logical. Enable Chain of Verification.
Empirically degrades accuracy — provided for research only. Default
\code{FALSE}.}

\item{chain_of_thought}{Logical. Enable chain-of-thought reasoning. Default
\code{FALSE}.}

\item{step_back_prompt}{Logical. Enable step-back prompting. Default
\code{FALSE}.}

\item{context_prompt}{Logical. Add expert context to prompts. Default
\code{FALSE}.}

\item{thinking_budget}{Integer. Extended thinking token budget (0 = off).
Default \code{0L}.}

\item{example1, example2, example3, example4, example5, example6}{Optional
few-shot example strings. Empirically degrades accuracy — provided for
research only.}

\item{filename}{Character or \code{NULL}. Output CSV filename. Default \code{NULL}.}

\item{save_directory}{Character or \code{NULL}. Directory to save results.
Default \code{NULL}.}

\item{model_source}{Character. Provider hint for single-model mode:
\code{"auto"}, \code{"openai"}, \code{"anthropic"}, \code{"google"}, \code{"mistral"},
\code{"huggingface"}, \code{"xai"}. Default \code{"auto"}.}

\item{max_categories}{Integer. Maximum number of categories when
\code{categories = "auto"}. Default \code{12L}.}

\item{categories_per_chunk}{Integer. Categories extracted per chunk when
\code{categories = "auto"}. Default \code{10L}.}

\item{divisions}{Integer. Number of data chunks when \code{categories = "auto"}.
Default \code{10L}.}

\item{research_question}{Character or \code{NULL}. Optional research context.
Default \code{NULL}.}

\item{models}{A list of model specifications for multi-model ensemble mode.
Each element is either a 3-element character vector
\code{c("model", "provider", "api_key")} or a 4-element list
\code{list("model", "provider", "api_key", list(creativity = 0.5))}.
When \code{models} is supplied, \code{api_key} and \code{user_model} are ignored.}

\item{consensus_threshold}{Character or numeric. Agreement threshold for
ensemble mode. Options: \code{"unanimous"} (default, 100\%), \code{"majority"}
(50\%), \code{"two-thirds"} (67\%), or a numeric value between 0 and 1.}

\item{survey_question}{Character. The survey question text (used when
\code{categories = "auto"}). Default \code{""}.}

\item{use_json_schema}{Logical. Use JSON schema for structured output.
Default \code{TRUE}.}

\item{max_workers}{Integer or \code{NULL}. Max parallel workers. \code{NULL} = auto.
Default \code{NULL}.}

\item{fail_strategy}{Character. How to handle failures: \code{"partial"}
(default) or \code{"strict"}.}

\item{max_retries}{Integer. Max retries per API call. Default \code{5L}.}

\item{batch_retries}{Integer. Max retries for batch-level failures.
Default \code{2L}.}

\item{retry_delay}{Numeric. Seconds between retries. Default \code{1.0}.}

\item{row_delay}{Numeric. Seconds between processing each row (useful for
rate limiting). Default \code{0.0}.}

\item{pdf_dpi}{Integer. DPI for PDF page rendering. Default \code{150L}.}

\item{auto_download}{Logical. Auto-download Ollama models. Default \code{FALSE}.}

\item{add_other}{Logical or \code{"prompt"}. Controls auto-addition of an
"Other" catch-all category. \code{"prompt"} (default) asks interactively —
in non-interactive sessions this silently defaults to "no". \code{TRUE}
silently adds "Other". \code{FALSE} never adds it.}

\item{check_verbosity}{Logical. Check whether each category has a
description and examples (1 API call). Default \code{TRUE}.}
}
\value{
A \code{data.frame} with one row per input item and classification
columns. In single-model mode the columns are the category names. In
ensemble mode additional \verb{consensus_*} and \verb{agreement_*} columns are
included.
}
\description{
Wraps the Python \code{catllm.classify()} function. Supports both single-model
and multi-model (ensemble) classification.
}
\examples{
\dontrun{
# Single-model classification
results <- classify(
  input_data  = c("I love this!", "Terrible service.", "It was okay."),
  categories  = c("Positive", "Negative", "Neutral"),
  description = "Customer feedback",
  api_key     = Sys.getenv("OPENAI_API_KEY")
)

# Multi-model ensemble
results <- classify(
  input_data  = df$responses,
  categories  = c("Positive", "Negative", "Neutral"),
  models      = list(
    c("gpt-4o",              "openai",    Sys.getenv("OPENAI_API_KEY")),
    c("claude-sonnet-4-5-20250929", "anthropic", Sys.getenv("ANTHROPIC_API_KEY"))
  ),
  consensus_threshold = "unanimous"
)
}

}
